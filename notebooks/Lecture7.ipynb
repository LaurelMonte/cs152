{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7 notebook\n",
    "## CS152 September 26, 2018  Neil Rhodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $f(x_1, x_2, x_3, w_1, w_2, b) = \\frac{1}{1 + e^{-(w_1x_1 + w_2x_2 + b)}}$, we want to find $\\frac{\\partial{f}}{{w_1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define this as a one-layer neural network with sigmoid activation function. We can use NumPy to do matrix calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid_np(a):\n",
    "    return 1/(1+np.exp(-a))\n",
    "\n",
    "def sigmoid_np_prime(a):\n",
    "    return sigmoid_np(a)*(1-sigmoid_np(a))\n",
    "\n",
    "def relu_np(z):\n",
    "    return np.maximum(0 ,z)\n",
    "\n",
    "def relu_np_prime(z):\n",
    "    # anything in z <= 0 maps to 0. Anything > 0 maps to 1\n",
    "    return np.where(z <= 0, 0, 1)\n",
    "\n",
    "def weightedsum_np(A, W, b):\n",
    "    return np.matmul(W, A) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Differentiation\n",
    "Here, we calculate via numeric differentation, the value $\\frac{\\partial{f}}{{w_1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: [[0.73105858]]\n",
      "df/dw1: [[0.19661189]]\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'x': np.array([[3.0], [-2.0]]),\n",
    "    'W1': np.array([[1.0, 2.0]]),\n",
    "    'b1':np.array([2.0]),\n",
    "}\n",
    "\n",
    "def f(params):\n",
    "    v = {}\n",
    "    v['layer1z'] = weightedsum_np(params['x'], params['W1'], params['b1'])\n",
    "    v['layer1a'] = sigmoid_np(v['layer1z'])\n",
    "    return v['layer1a'], v                      \n",
    "\n",
    "f_out, values = f(params)\n",
    "print(f'f: {f_out}')\n",
    "\n",
    "# Clone params\n",
    "params_epsilon = dict(params)\n",
    "epsilon = .000001\n",
    "params['W1'][0] += epsilon\n",
    "fepsilon, _ = f(params_epsilon)\n",
    "\n",
    "df_dw1 = (fepsilon-f_out)/epsilon\n",
    "print(f'df/dw1: {df_dw1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the output value of each node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "Now, we'll do backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss/dW1: [[ 0.5898358 ]\n",
      " [-0.39322387]]\n",
      "dloss/db1: [0.19661193]\n",
      "dloss/dw_1: [0.5898358]\n"
     ]
    }
   ],
   "source": [
    "dloss_dlayer1a = np.array([1]) # since f = layer1a, df/dlayer1a = 1\n",
    "dloss_dlayer1z = np.matmul(dloss_dlayer1a, sigmoid_np_prime(values['layer1z']))\n",
    "dloss_dW1 = np.matmul(params['x'], dloss_dlayer1z.transpose())\n",
    "dloss_db1 = dloss_dlayer1z\n",
    "\n",
    "print(f'dloss/dW1: {dloss_dw1}')\n",
    "print(f'dloss/db1: {dloss_db1}')\n",
    "\n",
    "print(f'dloss/dw_1: {dloss_dw1[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation using PyTorch\n",
    "Let's look at how to differentiate using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(...) =  Variable containing:\n",
      " 0.7311\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "df/dw1: Variable containing:\n",
      " 0.5898\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "df/dw2: Variable containing:\n",
      "-0.3932\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "df/db: Variable containing:\n",
      " 0.1966\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "\n",
    "def f(x1, x2, w1, w2, b):\n",
    "    weightedSum = x1*w1 + x2*w2 + b\n",
    "    return 1/(1+torch.exp(-weightedSum))\n",
    "\n",
    "x1 = Variable(Tensor([3.0])) # in pyTorch ≥0.4: x1 = Tensor([3.0])\n",
    "w1 = Variable(Tensor([1.0]), requires_grad=True) # in pyTorch ≥0.4: w1 = Tensor([1.0], requires_grad=True)\n",
    "x2 = Variable(Tensor([-2.0]))\n",
    "w2 = Variable(Tensor([2.0]), requires_grad=True)\n",
    "b = Variable(Tensor([2.0]), requires_grad=True)\n",
    "\n",
    "result = f(x1, x2, w1, w2, b)\n",
    "print(\"f(...) = \", result)\n",
    "\n",
    "result.backward()\n",
    "print(f'df/dw1: {w1.grad}')\n",
    "print(f'df/dw2: {w2.grad:}')\n",
    "print(f'df/db: {b.grad:}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(...) =  Variable containing:\n",
      " 0.7311\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "df/dw: Variable containing:\n",
      " 0.5898\n",
      "-0.3932\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "df/db: Variable containing:\n",
      " 0.1966\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "\n",
    "def sigmoid_layer(w, x, b):\n",
    "    return sigmoid(torch.mm(w.t(), x)+ b)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def relu_layer(w, x, b):\n",
    "    return relu(torch.mm(w.t(), x)+ b)\n",
    "\n",
    "def relu(x):\n",
    "    return x.clamp(min=0)\n",
    "\n",
    "\n",
    "x = Variable(Tensor([[3.0], [-2.0]])) # shape: (2, 1)\n",
    "w = Variable(Tensor([[1.0], [2.0]]), requires_grad=True) # shape: (2, 1)\n",
    "b = Variable(Tensor([2.0]), requires_grad=True)\n",
    "\n",
    "a1 = sigmoid_layer(x, w, b)\n",
    "print(\"f(...) = \", a1)\n",
    "\n",
    "a1.backward()\n",
    "print(f'df/dw: {w.grad}')\n",
    "print(f'df/db: {b.grad:}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch4]",
   "language": "python",
   "name": "conda-env-pytorch4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
